ds_op:  {'exp_label': 'med'}
experiment repetition:  2  of  3

Sparse mode: False
device cuda:3
tensor type torch.FloatTensor
Epochs: 100
Batch size: 256
Starting with exp_op:
('model_class', <class 'siamese_graph.GCNSiamese2'>)
('model_args', {'h_ch': 64, 'out_ch': 64, 'drop_p': None, 'conv_layers_num': 9, 'conv_type': <class 'torch_geometric.nn.conv.le_conv.LEConv'>, 'conv_long_type': None, 'conv_long_args': [3], 'att_type': None, 'att_heads': 1, 'pool_type': <class 'siamese_graph.GlobalAttentionSelect'>, 'pool_att_ch': 32, 'pool_att_layers': 8, 'pool_final_relu': True, 'pool_ref': 'last', 'final_out_layers_num': 4, 'final_out_ch': 64, 'final_out_join': 'abs'})


loss_fn: BCEWithLogitsLoss()
loss_fn_aux_dict:
('soft_f1_loss', <function soft_f1_loss at 0x7f9b708f1dc0>)
('double_soft_f1_loss', <function double_soft_f1_loss at 0x7f9b708f1e50>)
('bound_f1_loss', <function bound_f1_loss at 0x7f9b708f1f70>)



===== Model:

 GCNSiamese2(
  (conv1): LEConv(38, 64)
  (conv2): LEConv(64, 64)
  (conv_out): LEConv(64, 64)
  (norm1): BatchNorm(64)
  (norm2): BatchNorm(64)
  (norm_out): BatchNorm(64)
  (conv3): LEConv(64, 64)
  (conv4): LEConv(64, 64)
  (conv5): LEConv(64, 64)
  (norm3): BatchNorm(64)
  (norm4): BatchNorm(64)
  (norm5): BatchNorm(64)
  (conv6): LEConv(64, 64)
  (conv7): LEConv(64, 64)
  (conv8): LEConv(64, 64)
  (norm6): BatchNorm(64)
  (norm7): BatchNorm(64)
  (norm8): BatchNorm(64)
  (pool): GlobalAttentionSelect(gate_nn=Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
    (3): ReLU()
    (4): Linear(in_features=32, out_features=32, bias=True)
    (5): ReLU()
    (6): Linear(in_features=32, out_features=32, bias=True)
    (7): ReLU()
    (8): Linear(in_features=32, out_features=32, bias=True)
    (9): ReLU()
    (10): Linear(in_features=32, out_features=32, bias=True)
    (11): ReLU()
    (12): Linear(in_features=32, out_features=32, bias=True)
    (13): ReLU()
    (14): Linear(in_features=32, out_features=1, bias=True)
    (15): ReLU()
  ), nn=None)
  (final_out): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=1, bias=True)
  )
)

===== Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

 ===== Train model. Execute time was:
Minutes: 149.10
Seconds: 8946.04

 ===== Test last_model. Execute time was:
Minutes: 0.05
Seconds: 2.85

 ===== Test best_model. Execute time was:
Minutes: 0.05
Seconds: 2.84

 ===== Test best_model_sa. Execute time was:
Minutes: 0.05
Seconds: 2.86

Saving raw metrics in: ../output_siamese/experiment_2/20-small-train_07020518/med_2-2/cooc-07020518_med_2-2_metrics

 ===== best_model:
Epoch: 42
                          val      test
-------------------  --------  --------
main loss            0.30808   0.340652
average              0.890372  0.874962
roc_auc              0.941863  0.929996
f1_score             0.864279  0.846895
c_at_1               0.866705  0.849021
f_05                 0.874334  0.855407
brier                0.904678  0.893488
soft_f1_loss         0.527433  0.53394
double_soft_f1_loss  0.5316    0.538374
bound_f1_loss        0.54674   0.551839

 ===== best_model_sa:
Epoch: 64
                          val      test
-------------------  --------  --------
main loss            0.310781  0.348649
average              0.890752  0.875841
roc_auc              0.94288   0.930608
f1_score             0.867989  0.852713
c_at_1               0.868416  0.851873
f_05                 0.870181  0.851072
brier                0.904296  0.892938
soft_f1_loss         0.521498  0.525813
double_soft_f1_loss  0.529025  0.535219
bound_f1_loss        0.540641  0.543607

 ===== last_model:
Epoch: 99
                          val      test
-------------------  --------  --------
main loss            0.358043  0.41266
average              0.879572  0.864318
roc_auc              0.936941  0.922358
f1_score             0.854023  0.839445
c_at_1               0.855106  0.839323
f_05                 0.858364  0.840307
brier                0.893427  0.880155
soft_f1_loss         0.521565  0.526635
double_soft_f1_loss  0.52712   0.533362
bound_f1_loss        0.54154   0.544914

 ===== Early stop model:
Epoch: 42
Early stop model deactivated

===== Max average in val: 0.890752 epoch 64

===== Optimize threshold for best_model:
Test: Original
0.8749615142374154
Val: Best th posible
('0.50', '0.20')
0.9024062659481235
Test: Optimized
0.8884155606708207
Test: Best th posible
('0.50', '0.15')
0.8884912948885736

===== Optimize threshold for best_model_sa:
Test: Original
0.8758408727806681
Val: Best th posible
('0.55', '0.25')
0.9040879588926691
Test: Optimized
0.8876462779890915
Test: Best th posible
('0.60', '0.20')
0.8884450131194533

==========
Max gpu_usage: 2353

 ===== Total time for cooc-07020518_med_2-2. Execute time was:
Minutes: 150.71
Seconds: 9042.39
